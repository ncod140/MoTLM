{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824481ba",
   "metadata": {},
   "source": [
    "#### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28404bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "import numba\n",
    "import pandas\n",
    "from scipy import special\n",
    "\n",
    "import torch\n",
    "from torch import linalg as LA\n",
    "from torch import digamma, log, sqrt\n",
    "from torch.special import gammaln, ndtr\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from scipy.stats import qmc\n",
    "from scipy.special import gammaincinv\n",
    "\n",
    "def Matrix_gradient_gamma(n_row, x_halton, K_p, TAU_p, K, TAU, Mat_dists, weights, Mat_Theta, Mat_Loss_in, Loss_ext, Lambda):\n",
    "    n_points, n_samples = Mat_dists.shape\n",
    "    \n",
    "    BETA = gammaincinv(K, x_halton) / TAU\n",
    "    THETA_DER_OVER_K = (torch.log(TAU) - torch.digamma(K)).reshape(n_points,1) * Mat_Theta + torch.mean(torch.log(BETA.reshape(n_row, n_points, 1)) * (Mat_dists * torch.ones(n_row, 1, 1) <= BETA.reshape(n_row, n_points, 1)), axis=0)\n",
    "    THETA_DER_OVER_TAU = (K / TAU).reshape(n_points,1) * (Mat_Theta - special.gdtrc(TAU.reshape(n_points,1).numpy(), (K+1).reshape(n_points,1).numpy(), Mat_dists.numpy()))\n",
    "    \n",
    "    Mat_Theta_complement_modified = torch.where((1 - Mat_Theta) == 0, 1e-4, (1 - Mat_Theta))\n",
    "    Mat_Theta_prod = torch.prod((torch.zeros(n_points, 1, n_samples) + Mat_Theta_complement_modified), axis=1) / Mat_Theta_complement_modified\n",
    "\n",
    "    vet_K = THETA_DER_OVER_K * (Mat_Loss_in - Mat_Theta_prod * (torch.ones(n_points,1) * Loss_ext))\n",
    "    vet_TAU = THETA_DER_OVER_TAU * (Mat_Loss_in - Mat_Theta_prod * (torch.ones(n_points,1) * Loss_ext))\n",
    "    \n",
    "    weights = weights.reshape(weights.shape[0],)\n",
    "    \n",
    "    GRADIENT_K = torch.mean(vet_K * (torch.ones(n_points,1) * weights), axis=1) + (1 / Lambda) * ((K - K_p) * torch.polygamma(1, K) + ((TAU_p - TAU) / TAU))\n",
    "    GRADIENT_TAU = torch.mean(vet_TAU * (torch.ones(n_points,1) * weights), axis=1) + (1 / Lambda) * ((K_p / TAU) - ((K * TAU_p) / TAU**2))\n",
    "    return GRADIENT_K, GRADIENT_TAU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b13c85-0f9d-4141-b4f4-4297052b036c",
   "metadata": {},
   "source": [
    "#### 2. Mixture Of Linear Experts In Many Regions (Risk Minimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b00a754-69b4-449b-919e-aaa4445aa7e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1f550-bca5-4456-bcab-bf4231c81824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_binary_clf_gn(initial_parameters, X, y, Mat_dists, learning_rate, Lambda, max_iters, weights, print_flag):\n",
    "    # data shape and number of vicinity points\n",
    "    n_samples, n_features = X.shape\n",
    "    n_points = Mat_dists.shape[0]\n",
    "    \n",
    "    # get initial_parameters\n",
    "    gaussian_params, gamma_priors, gamma_params = initial_parameters\n",
    "    W, MU, SIGMA, w_ext, mu_ext, sigma_ext = gaussian_params\n",
    "    W.requires_grad_()\n",
    "    MU.requires_grad_()\n",
    "    SIGMA.requires_grad_()\n",
    "    w_ext.requires_grad_()\n",
    "    mu_ext.requires_grad_()\n",
    "    sigma_ext.requires_grad_()\n",
    "    K_p, TAU_p = gamma_priors\n",
    "    K, TAU = gamma_params\n",
    "\n",
    "    optimizer = torch.optim.NAdam([{'params': [W, MU, w_ext, mu_ext]},\n",
    "                                   {'params': [SIGMA, sigma_ext], 'lr': 0.05 * learning_rate},\n",
    "                                   {'params': [K, TAU], 'lr': 20 * learning_rate}\n",
    "                                  ], lr=learning_rate)\n",
    "\n",
    "    terminated_conv, old_risk_bound = 0, 1000\n",
    "    norm_X_square = LA.norm(X, dim=1)**2\n",
    "    Max_, max_ = 1000 * torch.ones(n_points), torch.ones(n_points)\n",
    "\n",
    "    dim, n_row = 1, 60\n",
    "    sampler = qmc.Halton(dim, scramble=True, seed=0)\n",
    "    x_halton = sampler.random(n_row)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute Mat_Theta\n",
    "        Mat_Theta = torch.from_numpy(special.gdtrc((TAU.reshape(n_points,1)).numpy(), (K.reshape(n_points, 1)).numpy(), Mat_dists.numpy()))\n",
    "\n",
    "        # Mat_Loss_in, Loss_ext\n",
    "        Mat_Loss_in = 1 - ndtr( (y.reshape(n_samples, 1) * (torch.matmul(X, W.T) + MU)) / sqrt(SIGMA**2 + norm_X_square.reshape(n_samples, 1)) ).T\n",
    "        Loss_ext = 1 - ndtr( (y * (torch.matmul(X, w_ext) + mu_ext)) / sqrt(sigma_ext**2 + norm_X_square) )\n",
    "        \n",
    "        # compute KL divergence\n",
    "        kl = - math.log(torch.prod(SIGMA) * sigma_ext) + 0.5 * (torch.sum(SIGMA**2) + sigma_ext**2 - (n_points + 1))\n",
    "        kl = kl + 0.5 * ( (LA.norm(W, axis=1)**2).sum() + (MU**2).sum() + LA.norm(w_ext)**2 + mu_ext**2 )\n",
    "        kl = kl + ( (K - K_p) * digamma(K) + gammaln(K_p) - gammaln(K) + K_p * log(TAU / TAU_p) + K * ((TAU_p - TAU) / TAU) ).sum()\n",
    "\n",
    "        # compute risk_bound\n",
    "        loss = torch.sum(Mat_Theta * Mat_Loss_in, axis=0) + torch.prod(1 - Mat_Theta, axis=0) * Loss_ext\n",
    "        risk_bound = torch.mean(loss * weights) + (1 / Lambda) *  kl\n",
    "        \n",
    "        # compute gradient over gaussian parameters\n",
    "        risk_bound.backward() \n",
    "        \n",
    "        # compute gradient over gamma parameters\n",
    "        K.grad, TAU.grad = Matrix_gradient_gamma(n_row, x_halton, K_p, TAU_p, K, TAU, Mat_dists, weights, Mat_Theta, Mat_Loss_in, Loss_ext, Lambda)\n",
    "            \n",
    "        optimizer.step()                                                              # update gradient\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            K[:] = torch.clamp(K, min=K_p, max=Max_)                                  # contraint : K_p < K < Max_\n",
    "            TAU[:] = torch.clamp(TAU, min=TAU_p, max=Max_)                            # contraint : TAU_p < TAU < Max_\n",
    "            \n",
    "            SIGMA[:] = torch.clamp(torch.abs(SIGMA), max=max_)                        # contraint on 0 < SIGMA < 1\n",
    "            sigma_ext[:] = torch.clamp(torch.abs(sigma_ext), max=torch.tensor([1.0])) # contraint on 0 < sigma_ext < 1\n",
    "\n",
    "            # gradients and risk bound stop criterion\n",
    "            if (i+1)%40 == 0 :\n",
    "                if ((risk_bound / old_risk_bound) >= 0.95) or ((K / TAU**2).sum() > n_points) :\n",
    "                    terminated_conv = 1\n",
    "                    break;\n",
    "                else:\n",
    "                    old_risk_bound = risk_bound\n",
    "                        \n",
    "    if print_flag:\n",
    "        if terminated_conv != 0:\n",
    "            print(f'Converged in {i+1} iteration...')\n",
    "        else:\n",
    "            print(f'Terminated in {i+1} iteration... because maximum number of iterations has been exceeded')\n",
    "            \n",
    "    return (W.detach().numpy(), MU.detach().numpy(), SIGMA.detach().numpy(), w_ext.detach().numpy(), mu_ext.detach().numpy()[0], sigma_ext.detach().numpy()[0]), (K.numpy(), TAU.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241ce2b-23f0-4a05-819b-18fb5c9ef3eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d3082-67d5-4865-ac57-05bb16b22b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_reg_gn(initial_parameters, X, y, Mat_dists, learning_rate, Lambda, max_iters, weights, print_flag):\n",
    "    # data shape and number of vicinity points\n",
    "    n_samples, n_features = X.shape\n",
    "    n_points = Mat_dists.shape[0]\n",
    "    \n",
    "    # get initial_parameters\n",
    "    gaussian_params, gamma_priors, gamma_params = initial_parameters\n",
    "    W, RHO, MU, SIGMA, w_ext, rho_ext, mu_ext, sigma_ext = gaussian_params\n",
    "    W.requires_grad_()\n",
    "    RHO.requires_grad_()\n",
    "    MU.requires_grad_()\n",
    "    SIGMA.requires_grad_()\n",
    "    w_ext.requires_grad_()\n",
    "    rho_ext.requires_grad_()\n",
    "    mu_ext.requires_grad_()\n",
    "    sigma_ext.requires_grad_()\n",
    "    K_p, TAU_p = gamma_priors\n",
    "    K, TAU = gamma_params\n",
    "\n",
    "    optimizer = torch.optim.NAdam([{'params': [W, MU, w_ext, mu_ext]},\n",
    "                                   {'params': [RHO, SIGMA, rho_ext, sigma_ext], 'lr': 0.05 * learning_rate},\n",
    "                                   {'params': [K, TAU], 'lr': 20 * learning_rate}\n",
    "                                  ], lr=learning_rate)\n",
    "\n",
    "    terminated_conv, old_risk_bound = 0, 1000\n",
    "    norm_X_square = LA.norm(X, dim=1)**2\n",
    "    Max_, max_ = 1000 * torch.ones(n_points), torch.ones(n_points)\n",
    "\n",
    "    dim, n_row = 1, 60\n",
    "    sampler = qmc.Halton(dim, scramble=True, seed=0)\n",
    "    x_halton = sampler.random(n_row)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute Mat_Theta\n",
    "        Mat_Theta = torch.from_numpy(special.gdtrc((TAU.reshape(n_points,1)).numpy(), (K.reshape(n_points, 1)).numpy(), Mat_dists.numpy()))\n",
    "        \n",
    "        # Mat_Loss_in, Loss_ext\n",
    "        Mat_Loss_in = (norm_X_square.reshape(n_samples, 1) * RHO**2 + SIGMA**2 + ((torch.matmul(X, W.T) + MU) - y.reshape(n_samples, 1))**2).T\n",
    "        Loss_ext = norm_X_square * rho_ext**2 + sigma_ext**2 + (torch.matmul(X, w_ext) + mu_ext - y)**2\n",
    "\n",
    "        # compute KL divergence\n",
    "        kl = - n_features * math.log(torch.prod(RHO) * rho_ext) - math.log(torch.prod(SIGMA) * sigma_ext) + 0.5 * (n_features * (torch.sum(RHO**2) + rho_ext**2) + (torch.sum(SIGMA**2) + sigma_ext**2) - ((n_features + 1) * n_points + (n_features + 1)))\n",
    "        kl = kl + 0.5 * ( (LA.norm(W, axis=1)**2).sum() + (MU**2).sum() + LA.norm(w_ext)**2 + mu_ext**2 )\n",
    "        kl = kl + ( (K - K_p) * digamma(K) + gammaln(K_p) - gammaln(K) + K_p * log(TAU / TAU_p) + K * ((TAU_p - TAU) / TAU) ).sum()\n",
    "        \n",
    "        # compute risk_bound\n",
    "        loss = torch.sum(Mat_Theta * Mat_Loss_in, axis=0) + torch.prod(1 - Mat_Theta, axis=0) * Loss_ext\n",
    "        risk_bound = torch.mean(loss * weights) + (1 / Lambda) * kl\n",
    "        \n",
    "        # compute gradient over gaussian parameters\n",
    "        risk_bound.backward()  \n",
    "        \n",
    "        # compute gradient over gamma parameters\n",
    "        K.grad, TAU.grad = Matrix_gradient_gamma(n_row, x_halton, K_p, TAU_p, K, TAU, Mat_dists, weights, Mat_Theta, Mat_Loss_in, Loss_ext, Lambda)\n",
    "            \n",
    "        optimizer.step()                                                              # update gradient\n",
    "\n",
    "        with torch.no_grad():\n",
    "            K[:] = torch.clamp(K, min=K_p, max=Max_)                                  # contraint on K_p < K < Max_\n",
    "            TAU[:] = torch.clamp(TAU, min=TAU_p, max=Max_)                            # contraint on TAU_p < TAU < Max_\n",
    "\n",
    "            RHO[:] = torch.clamp(torch.abs(RHO), max=max_)                            # contraint on RHO>0\n",
    "            SIGMA[:] = torch.clamp(torch.abs(SIGMA), max=max_)                        # contraint on SIGMA>0\n",
    "            rho_ext[:] = torch.clamp(torch.abs(rho_ext), max=torch.tensor([1.0]))     # contraint on rho_ext>0\n",
    "            sigma_ext[:] = torch.clamp(torch.abs(sigma_ext), max=torch.tensor([1.0])) # contraint on sigma_ext>0\n",
    "\n",
    "            # gradients and risk bound stop criterion\n",
    "            if (i+1)%40 == 0 :\n",
    "                if ((risk_bound / old_risk_bound) >= 0.95) or ((K / TAU**2).sum() > n_points) :\n",
    "                    terminated_conv = 1\n",
    "                    break;\n",
    "                else:\n",
    "                    old_risk_bound = risk_bound\n",
    "                    \n",
    "    if print_flag:\n",
    "        if terminated_conv != 0:\n",
    "            print(f'Converged in {i+1} iteration...')\n",
    "        else:\n",
    "            print(f'Terminated in {i+1} iteration... because maximum number of iterations has been exceeded') \n",
    "\n",
    "    gaussian_params = (W.detach().numpy(), RHO.detach().numpy(), MU.detach().numpy(), SIGMA.detach().numpy(), w_ext.detach().numpy(), rho_ext.detach().numpy()[0], mu_ext.detach().numpy()[0], sigma_ext.detach().numpy()[0])\n",
    "    gamma_params = (K.numpy(), TAU.numpy())\n",
    "    \n",
    "    return gaussian_params, gamma_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24572768",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3. Computation of vicinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66fb4f-74ac-4198-a368-66dc3592b00b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def num_and_cat_features(dataset, print_var = False):\n",
    "    category_types_list = []\n",
    "    colNames = dataset.columns.values.tolist()\n",
    "    for colName in colNames:\n",
    "        if dataset[colName].dtypes == 'object' or dataset[colName].dtype.name == 'category':\n",
    "            category_types_list.append(colName)\n",
    "\n",
    "    numeric_types_list = pandas.Series(dataset.columns.drop(category_types_list, errors='ignore'))\n",
    "    if print_var == True:\n",
    "        print('\\n Numeric variables: \\n', [i for i in numeric_types_list])\n",
    "        print('\\n Category variables: \\n', category_types_list)\n",
    "    return numeric_types_list, category_types_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b8d8c9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_data_range(df_xtrain=None, data_1=None, data_2=None):\n",
    "    columns = df_xtrain.columns\n",
    "    numeric_vars, category_vars = num_and_cat_features(df_xtrain, print_var = False)\n",
    "    xtrain_range = pandas.DataFrame(columns = columns, dtype = float)\n",
    "    xtrain_range.loc[0,numeric_vars] = numpy.ptp(df_xtrain[numeric_vars], axis=0)\n",
    "    xtrain_range[category_vars] = 1\n",
    "    \n",
    "    if data_1 is None:\n",
    "        return xtrain_range.values[0]\n",
    "    elif data_2 is None:\n",
    "        data_1_range = pandas.DataFrame(columns = columns, dtype = float)\n",
    "        data_1_range.loc[0,numeric_vars] = numpy.ptp(data_1[numeric_vars], axis=0)\n",
    "        data_1_range[category_vars] = 1\n",
    "        return xtrain_range.values[0], data_1_range.values[0]\n",
    "    else:\n",
    "        data_1_range = pandas.DataFrame(columns = columns, dtype = float)\n",
    "        data_1_range.loc[0,numeric_vars] = numpy.ptp(data_1[numeric_vars], axis=0)\n",
    "        data_1_range[category_vars] = 1\n",
    "        \n",
    "        data_2_range = pandas.DataFrame(columns = columns, dtype = float)\n",
    "        data_2_range.loc[0,numeric_vars] = numpy.ptp(data_2[numeric_vars], axis=0)\n",
    "        data_2_range[category_vars] = 1\n",
    "        return xtrain_range.values[0], data_1_range.values[0], data_2_range.values[0]\n",
    "\n",
    "def distance_processing(df_x0=None, df_xtrain=None, data_1=None, data_2=None):\n",
    "    numeric_vars, category_vars = num_and_cat_features(df_xtrain, print_var = False)\n",
    "    for col in category_vars:\n",
    "        df_xtrain.loc[:, col] = (df_xtrain[col] == df_x0[col].values[0]).values.astype(int)\n",
    "    \n",
    "    if data_1 is None:\n",
    "        df_x0.loc[:, category_vars] = 1\n",
    "        return df_x0.values[0], df_xtrain.values\n",
    "    elif data_2 is None:\n",
    "        for col in category_vars:\n",
    "            data_1.loc[:, col] = (data_1[col] == df_x0[col].values[0]).values.astype(int)\n",
    "        df_x0.loc[:, category_vars] = 1\n",
    "        return df_x0.values[0], df_xtrain.values, data_1.values\n",
    "    else:\n",
    "        for col in category_vars:\n",
    "            data_1.loc[:, col] = (data_1[col] == df_x0[col].values[0]).values.astype(int)\n",
    "            data_2.loc[:, col] = (data_2[col] == df_x0[col].values[0]).values.astype(int)\n",
    "        df_x0.loc[:, category_vars] = 1\n",
    "        return df_x0.values[0], df_xtrain.values, data_1.values, data_2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94582059",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True, fastmath=True)\n",
    "def fast_euclidean_dist(x_1d, y_1d):\n",
    "    dist = [(a - b)**2 for a, b in list(zip(x_1d, y_1d))]\n",
    "    dist = numpy.array(dist)\n",
    "    return numpy.sqrt(numpy.sum(dist, axis=0))\n",
    "\n",
    "@numba.jit(nopython=True, fastmath=True)\n",
    "def fast_gower_dist(x_1d, y_1d, z_1d):\n",
    "    dist = [numpy.abs(a - b) for a, b in list(zip(x_1d, y_1d))]\n",
    "    dist = numpy.array(dist) / z_1d\n",
    "    return numpy.sum(dist, axis=0) / len(x_1d)\n",
    "\n",
    "@numba.jit(nopython=True, fastmath=True)\n",
    "def compute_distance(dist_type='gower', x0=None, X=None, X_range=None):\n",
    "    distance = numpy.zeros(X.shape[0])\n",
    "    \n",
    "    if dist_type=='gower': \n",
    "        for i in numba.prange(X.shape[0]):\n",
    "            distance[i] = fast_gower_dist(x0, X[i], X_range)\n",
    "    elif dist_type=='euclidean':\n",
    "        for i in numba.prange(X.shape[0]):\n",
    "            distance[i] = fast_euclidean_dist(x0, X[i])\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0afaca7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vicinity(dist_type='gower', df_x0=None, df_xtrain=None, df_xtest_1=None, df_xtest_2=None):\n",
    "    \n",
    "    if df_xtest_1 is None:\n",
    "        xtrain_range = get_data_range(df_xtrain=df_xtrain, data_1=None, data_2=None)\n",
    "        x0, xtrain = distance_processing(df_x0=df_x0, df_xtrain=df_xtrain, data_1=None, data_2=None)\n",
    "        return compute_distance(dist_type=dist_type, x0=x0, X=xtrain, X_range=xtrain_range)\n",
    "    \n",
    "    elif df_xtest_2 is None:\n",
    "        xtrain_range = get_data_range(df_xtrain=df_xtrain, data_1=None, data_2=None)\n",
    "        x0, xtrain, xtest = distance_processing(df_x0=df_x0, df_xtrain=df_xtrain, data_1=df_xtest_1, data_2=None)\n",
    "        dist_train = compute_distance(dist_type=dist_type, x0=x0, X=xtrain, X_range=xtrain_range)\n",
    "        dist_test = compute_distance(dist_type=dist_type, x0=x0, X=xtest, X_range=xtrain_range)\n",
    "        return dist_train, dist_test\n",
    "    \n",
    "    else:\n",
    "        xtrain_range = get_data_range(df_xtrain=df_xtrain, data_1=None, data_2=None)\n",
    "        x0, xtrain, xtest_1, xtest_2 = distance_processing(df_x0=df_x0, df_xtrain=df_xtrain, data_1=df_xtest_1, data_2=df_xtest_2)\n",
    "        dist_train = compute_distance(dist_type=dist_type, x0=x0, X=xtrain, X_range=xtrain_range)\n",
    "        dist_test_1 = compute_distance(dist_type=dist_type, x0=x0, X=xtest_1, X_range=xtrain_range)\n",
    "        dist_test_2 = compute_distance(dist_type=dist_type, x0=x0, X=xtest_2, X_range=xtrain_range)\n",
    "        return dist_train, dist_test_1, dist_test_2\n",
    "\n",
    "def set_of_vicinity(dist_type='gower', df_X0=None, df_xtrain=None, df_xtest_1=None, df_xtest_2=None):\n",
    "    DIST_TRAIN, DIST_TEST_1, DIST_TEST_2 = [], [], []\n",
    "    idx = df_X0.index\n",
    "    \n",
    "    if df_xtest_1 is None:\n",
    "        for i in range(len(idx)):\n",
    "            DIST_TRAIN.append(vicinity(dist_type=dist_type, df_x0=df_X0.loc[idx[i]:idx[i]], df_xtrain=df_xtrain))\n",
    "        return numpy.array(DIST_TRAIN)\n",
    "    \n",
    "    elif df_xtest_2 is None:\n",
    "        for i in range(len(idx)):\n",
    "            dist_train, dist_test = vicinity(dist_type=dist_type, df_x0=df_X0.loc[idx[i]:idx[i]], df_xtrain=df_xtrain, df_xtest_1=df_xtest_1)\n",
    "            DIST_TRAIN.append(dist_train)\n",
    "            DIST_TEST_1.append(dist_test)\n",
    "        return numpy.array(DIST_TRAIN), numpy.array(DIST_TEST_1)\n",
    "            \n",
    "    else:\n",
    "        for i in range(len(idx)):\n",
    "            dist_train, dist_test_1, dist_test_2 = vicinity(dist_type=dist_type, df_x0=df_X0.loc[idx[i]:idx[i]], df_xtrain=df_xtrain, df_xtest_1=df_xtest_1, df_xtest_2=df_xtest_2)\n",
    "            DIST_TRAIN.append(dist_train)\n",
    "            DIST_TEST_1.append(dist_test_1)\n",
    "            DIST_TEST_2.append(dist_test_2)\n",
    "        return numpy.array(DIST_TRAIN), numpy.array(DIST_TEST_1), numpy.array(DIST_TEST_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
