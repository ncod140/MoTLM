{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824481ba",
   "metadata": {},
   "source": [
    "#### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cedf78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Importing libraries\n",
    "import math\n",
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.ticker as mtick\n",
    "from collections import Counter\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c569bb60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c83681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncouping_x_y(data, target_name):\n",
    "    # encode categories variables\n",
    "    ord_enc = OrdinalEncoder()\n",
    "    data[target_name] = ord_enc.fit_transform(data[[target_name]])\n",
    "    y = data[target_name].astype('int')\n",
    "    y = numpy.array(y)\n",
    "    y = numpy.where(y == 0, -1, 1)\n",
    "    data.drop([target_name], axis=1, inplace=True, errors='ignore')\n",
    "    return data, y\n",
    "\n",
    "# get imbalanced flag\n",
    "def get_imbalanced_flag(y_data):\n",
    "    unique, counts = numpy.unique(y_data, return_counts=True)\n",
    "    ss = numpy.asarray((unique, counts)).T\n",
    "    if len(unique) > 1:\n",
    "        imbalanced = True if min(ss[0,1], ss[1,1]) / (ss[0,1] + ss[1,1]) < 0.3 else False\n",
    "    else :\n",
    "        imbalanced = False\n",
    "        \n",
    "    return imbalanced\n",
    "\n",
    "def uncouping_x_y_reg(data, target_name, synthetic_data_flag=False):\n",
    "    y = pandas.DataFrame(data[target_name])\n",
    "    numeric_vars, category_vars = num_and_cat_features(y, print_var = False)\n",
    "    y = y.values if synthetic_data_flag==True else data_scaling(scale_type='Standardscaler', numeric_vars=numeric_vars, xtrain=y, data_1=None, data_2=None, data_3=None).values\n",
    "    data.drop([target_name], axis=1, inplace=True, errors='ignore')\n",
    "    return data, y.reshape(y.size)\n",
    "\n",
    "def VIF(X_train, print_flag=False):\n",
    "    def compute_VIF(X_train, print_flag=False):\n",
    "        # VIF dataframe\n",
    "        vif_data = pandas.DataFrame()\n",
    "        vif_data[\"feature\"] = X_train.columns\n",
    "\n",
    "        # calculating VIF for each feature\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(len(X_train.columns))]\n",
    "        if print_flag:\n",
    "            print(vif_data)\n",
    "        return vif_data\n",
    "\n",
    "    del_vars = []\n",
    "    vif_data = compute_VIF(X_train, print_flag=print_flag)\n",
    "    del_var = vif_data[vif_data[\"VIF\"]==max(vif_data[\"VIF\"])]['feature'].values[0] if max(vif_data[\"VIF\"]) >=10 else None\n",
    "    while(del_var is not None):\n",
    "        del_vars.append(del_var)\n",
    "        X_train.drop([del_var], axis=1, inplace=True, errors='ignore')\n",
    "        vif_data = compute_VIF(X_train, print_flag=print_flag)\n",
    "        del_var = vif_data[vif_data[\"VIF\"]==max(vif_data[\"VIF\"])]['feature'].values[0] if max(vif_data[\"VIF\"]) >=10 else None\n",
    "    \n",
    "    return del_vars\n",
    "\n",
    "# Generalisation of Mixture of experts toward many regions\n",
    "def selection_of_X0(X0_info=None, df_xtrain=None, ytrain=None):\n",
    "    assert X0_info is not None\n",
    "    \n",
    "    if type(X0_info) != pandas.core.frame.DataFrame:\n",
    "        idx = random.sample(range(0, df_xtrain.shape[0]), X0_info)\n",
    "        X0 = df_xtrain.iloc[idx,:]\n",
    "        df_xtrain = df_xtrain.drop(labels=idx, axis=0)\n",
    "        ytrain = numpy.delete(ytrain, idx, axis=0)\n",
    "    else:\n",
    "        X0 = X0_info\n",
    "        \n",
    "    return X0, df_xtrain, ytrain\n",
    "\n",
    "def reset_index_data(data_1=None, data_2=None, data_3=None, data_4=None):\n",
    "    if data_2 is None:\n",
    "        return data_1.reset_index(drop=True)\n",
    "    elif data_3 is None: \n",
    "        return data_1.reset_index(drop=True), data_2.reset_index(drop=True)\n",
    "    elif data_4 is None: \n",
    "        return data_1.reset_index(drop=True), data_2.reset_index(drop=True), data_3.reset_index(drop=True)\n",
    "    else:\n",
    "        return data_1.reset_index(drop=True), data_2.reset_index(drop=True), data_3.reset_index(drop=True), data_4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_and_cat_features(dataset, print_var = False):\n",
    "    category_types_list = []\n",
    "    colNames = dataset.columns.values.tolist()\n",
    "    for colName in colNames:\n",
    "        if dataset[colName].dtypes == 'object' or dataset[colName].dtype.name == 'category':\n",
    "            category_types_list.append(colName)\n",
    "\n",
    "    numeric_types_list = pandas.Series(dataset.columns.drop(category_types_list, errors='ignore'))\n",
    "    if print_var == True:\n",
    "        print('Numeric variables: \\n', [i for i in numeric_types_list])\n",
    "        print('Category variables: \\n', category_types_list)\n",
    "    return numeric_types_list, category_types_list\n",
    "\n",
    "def category_encoding(enc_type='OrdinalEncoder', xtrain=None, ytrain=None, data_1=None, data_2=None, data_3=None, smooth=300):\n",
    "    numeric_vars, category_vars = num_and_cat_features(xtrain, print_var = False)\n",
    "    if len(category_vars) != 0:\n",
    "        cat_xtrain = xtrain[category_vars]\n",
    "        if enc_type == 'OneHotEncoder':\n",
    "            #OneHotEncoder\n",
    "            enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first').fit(cat_xtrain)\n",
    "            # Create a DataFrame with the encoded columns\n",
    "            one_hot_df = pandas.DataFrame(enc.transform(cat_xtrain), columns=enc.get_feature_names_out(category_vars))\n",
    "            xtrain = pandas.concat([xtrain[numeric_vars], one_hot_df], axis=1)\n",
    "\n",
    "            if data_1 is None:\n",
    "                numeric_vars = xtrain.columns # a ete ajouter le 11 mars 2025 pour test\n",
    "                return numeric_vars, xtrain\n",
    "            elif data_2 is None:\n",
    "                one_hot_df = pandas.DataFrame(enc.transform(data_1[category_vars]), columns=enc.get_feature_names_out(category_vars))\n",
    "                data_1 = pandas.concat([data_1[numeric_vars], one_hot_df], axis=1)\n",
    "                numeric_vars = xtrain.columns # a ete ajouter le 11 mars 2025 pour test\n",
    "                return numeric_vars, xtrain, data_1\n",
    "            elif data_3 is None:\n",
    "                one_hot_df = pandas.DataFrame(enc.transform(data_1[category_vars]), columns=enc.get_feature_names_out(category_vars))\n",
    "                data_1 = pandas.concat([data_1[numeric_vars], one_hot_df], axis=1)\n",
    "\n",
    "                one_hot_df = pandas.DataFrame(enc.transform(data_2[category_vars]), columns=enc.get_feature_names_out(category_vars))\n",
    "                data_2 = pandas.concat([data_2[numeric_vars], one_hot_df], axis=1)\n",
    "                numeric_vars = xtrain.columns # a ete ajouter le 11 mars 2025 pour test\n",
    "                return numeric_vars, xtrain, data_1, data_2\n",
    "            else:\n",
    "                one_hot_df = pandas.DataFrame(enc.transform(data_1[category_vars]), columns=enc.get_feature_names_out(category_vars))\n",
    "                data_1 = pandas.concat([data_1[numeric_vars], one_hot_df], axis=1)\n",
    "\n",
    "                one_hot_df = pandas.DataFrame(enc.transform(data_2[category_vars]), columns=enc.get_feature_names_out(category_vars))\n",
    "                data_2 = pandas.concat([data_2[numeric_vars], one_hot_df], axis=1)\n",
    "\n",
    "                one_hot_df = pandas.DataFrame(enc.transform(data_3[category_vars]), columns=enc.get_feature_names_out(category_vars))\n",
    "                data_3 = pandas.concat([data_3[numeric_vars], one_hot_df], axis=1)\n",
    "                numeric_vars = xtrain.columns # a ete ajouter le 11 mars 2025 pour test\n",
    "                return numeric_vars, xtrain, data_1, data_2, data_3\n",
    "\n",
    "        elif enc_type == 'FrequencyEncoder':\n",
    "            #FrequencyEncoder\n",
    "            enc = CountFrequencyEncoder(encoding_method='frequency', variables=category_vars).fit(cat_xtrain)\n",
    "        elif enc_type == 'TargetEncoder':\n",
    "            # TargetEncoder\n",
    "            enc = TargetEncoder(cols=category_vars, smoothing=smooth).fit(cat_xtrain, ytrain)\n",
    "        else :\n",
    "            # OrdinalEncoder\n",
    "            enc = OrdinalEncoder(categories='auto', handle_unknown='use_encoded_value', unknown_value=-1).fit(cat_xtrain)\n",
    "\n",
    "        xtrain[category_vars] = enc.transform(cat_xtrain)\n",
    "        \n",
    "        if data_1 is None:\n",
    "            return numeric_vars, xtrain\n",
    "        elif data_2 is None:\n",
    "            data_1[category_vars] = enc.transform(data_1[category_vars])\n",
    "            return numeric_vars, xtrain, data_1\n",
    "        elif data_3 is None:\n",
    "            data_1[category_vars] = enc.transform(data_1[category_vars])\n",
    "            data_2[category_vars] = enc.transform(data_2[category_vars])\n",
    "            return numeric_vars, xtrain, data_1, data_2\n",
    "        else:\n",
    "            data_1[category_vars] = enc.transform(data_1[category_vars])\n",
    "            data_2[category_vars] = enc.transform(data_2[category_vars])\n",
    "            data_3[category_vars] = enc.transform(data_3[category_vars])\n",
    "            return numeric_vars, xtrain, data_1, data_2, data_3\n",
    "    else:\n",
    "        if data_1 is None:\n",
    "            return numeric_vars, xtrain\n",
    "        elif data_2 is None:\n",
    "            return numeric_vars, xtrain, data_1\n",
    "        elif data_3 is None:\n",
    "            return numeric_vars, xtrain, data_1, data_2\n",
    "        else:\n",
    "            return numeric_vars, xtrain, data_1, data_2, data_3\n",
    "        \n",
    "def data_scaling(scale_type='Minmax', numeric_vars=None, xtrain=None, data_1=None, data_2=None, data_3=None):\n",
    "    if scale_type == 'Minmax':\n",
    "        scaler = MinMaxScaler() \n",
    "    elif scale_type == 'Standardscaler':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MaxAbsScaler()\n",
    "\n",
    "    if len(numeric_vars) > 0:\n",
    "        xtrain[numeric_vars] = scaler.fit_transform(xtrain[numeric_vars])\n",
    "        \n",
    "        if data_1 is None:\n",
    "            return xtrain\n",
    "        elif data_2 is None:\n",
    "            data_1[numeric_vars] = scaler.transform(data_1[numeric_vars])\n",
    "            return xtrain, data_1\n",
    "        elif data_3 is None:\n",
    "            data_1[numeric_vars] = scaler.transform(data_1[numeric_vars])\n",
    "            data_2[numeric_vars] = scaler.transform(data_2[numeric_vars])\n",
    "            return xtrain, data_1, data_2\n",
    "        else:\n",
    "            data_1[numeric_vars] = scaler.transform(data_1[numeric_vars])\n",
    "            data_2[numeric_vars] = scaler.transform(data_2[numeric_vars])\n",
    "            data_3[numeric_vars] = scaler.transform(data_3[numeric_vars])\n",
    "            return xtrain, data_1, data_2, data_3\n",
    "    else:\n",
    "        if data_1 is None:\n",
    "            return xtrain\n",
    "        elif data_2 is None:\n",
    "            return xtrain, data_1\n",
    "        elif data_3 is None:\n",
    "            return xtrain, data_1, data_2\n",
    "        else:\n",
    "            return xtrain, data_1, data_2, data_3\n",
    "    \n",
    "def data_processing(xtrain=None, ytrain=None, xtest_1=None, xtest_2=None, xtest_3=None, enc_type='TargetEncoder', scale_type='Minmax', check_multicollinearity=False):\n",
    "    if xtest_1 is None:\n",
    "        numeric_vars, xtrain_enc = category_encoding(enc_type=enc_type, xtrain=xtrain, ytrain=ytrain, data_1=None, data_2=None, data_3=None, smooth=300)\n",
    "        if (check_multicollinearity == True) and (xtrain_enc[numeric_vars].shape[1] > 1):\n",
    "            del_vars = VIF(xtrain_enc[numeric_vars], print_flag=False)\n",
    "            xtrain_enc.drop(del_vars, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "            print(f'Check multicollinearity, Training_n_samples = {xtrain_enc.shape}')\n",
    "\n",
    "        new_numeric_vars = [feature for feature in xtrain_enc.columns if (numeric_vars == feature).sum() != 0]\n",
    "        xtrain_enc = data_scaling(scale_type=scale_type, numeric_vars=new_numeric_vars, xtrain=xtrain_enc, data_1=None, data_2=None, data_3=None)\n",
    "        return xtrain_enc\n",
    "    elif xtest_2 is None:\n",
    "        numeric_vars, xtrain_enc, xtest_enc_1 = category_encoding(enc_type=enc_type, xtrain=xtrain, ytrain=ytrain, data_1=xtest_1, data_2=None, data_3=None, smooth=300)\n",
    "        if (check_multicollinearity == True) and (xtrain_enc[numeric_vars].shape[1] > 1):\n",
    "            del_vars = VIF(xtrain_enc[numeric_vars], print_flag=False)\n",
    "            xtrain_enc.drop(del_vars, axis=1, inplace=True, errors='ignore')\n",
    "            xtest_enc_1.drop(del_vars, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "            print(f'Check multicollinearity, Training_n_samples = {xtrain_enc.shape}, Testing_n_samples = {xtest_enc_1.shape}')\n",
    "\n",
    "        new_numeric_vars = [feature for feature in xtrain_enc.columns if (numeric_vars == feature).sum() != 0]\n",
    "        xtrain_enc, xtest_enc_1 = data_scaling(scale_type=scale_type, numeric_vars=new_numeric_vars, xtrain=xtrain_enc, data_1=xtest_enc_1.copy(), data_2=None, data_3=None)\n",
    "        return xtrain_enc, xtest_enc_1\n",
    "    elif xtest_3 is None:\n",
    "        numeric_vars, xtrain_enc, xtest_enc_1, xtest_enc_2 = category_encoding(enc_type=enc_type, xtrain=xtrain, ytrain=ytrain, data_1=xtest_1, data_2=xtest_2, data_3=None, smooth=300)\n",
    "        if (check_multicollinearity == True) and (xtrain_enc[numeric_vars].shape[1] > 1):\n",
    "            del_vars = VIF(xtrain_enc[numeric_vars], print_flag=False)\n",
    "            xtrain_enc.drop(del_vars, axis=1, inplace=True, errors='ignore')\n",
    "            xtest_enc_1.drop(del_vars, axis=1, inplace=True, errors='ignore')\n",
    "            xtest_enc_2.drop(del_vars, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "            print(f'Check multicollinearity, Training_n_samples = {xtrain_enc.shape}, Validation_n_samples = {xtest_enc_1.shape}, Testing_n_samples = {xtest_enc_2.shape}')\n",
    "\n",
    "        new_numeric_vars = [feature for feature in xtrain_enc.columns if (numeric_vars == feature).sum() != 0]\n",
    "        xtrain_enc, xtest_enc_1, xtest_enc_2 = data_scaling(scale_type=scale_type, numeric_vars=new_numeric_vars, xtrain=xtrain_enc, data_1=xtest_enc_1.copy(), data_2=xtest_enc_2.copy(), data_3=None)\n",
    "        return xtrain_enc, xtest_enc_1, xtest_enc_2\n",
    "    else:\n",
    "        numeric_vars, xtrain_enc, xtest_enc_1, xtest_enc_2, xtest_enc_3 = category_encoding(enc_type=enc_type, xtrain=xtrain, ytrain=ytrain, data_1=xtest_1, data_2=xtest_2, data_3=xtest_3, smooth=300)\n",
    "        if (check_multicollinearity == True) and (xtrain_enc[numeric_vars].shape[1] > 1):\n",
    "            del_vars = VIF(xtrain_enc[numeric_vars], print_flag=False)\n",
    "            xtrain_enc.drop(del_vars, axis=1, inplace=True, errors='ignore')\n",
    "            xtest_enc_1.drop(del_vars, axis=1, inplace=True, errors='ignore')\n",
    "            xtest_enc_2.drop(del_vars, axis=1, inplace=True, errors='ignore')\n",
    "            xtest_enc_3.drop(del_vars, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "            print(f'Check multicollinearity, Training_n_samples = {xtrain_enc.shape}, Validation_n_samples = {xtest_enc_1.shape}, Testing_n_samples = {xtest_enc_2.shape}, Point_of_interest_n_samples = {xtest_enc_3.shape}')\n",
    "\n",
    "        new_numeric_vars = [feature for feature in xtrain_enc.columns if (numeric_vars == feature).sum() != 0]\n",
    "        xtrain_enc, xtest_enc_1, xtest_enc_2, xtest_enc_3 = data_scaling(scale_type=scale_type, numeric_vars=new_numeric_vars, xtrain=xtrain_enc, data_1=xtest_enc_1.copy(), data_2=xtest_enc_2.copy(), data_3=xtest_enc_3.copy())\n",
    "        return xtrain_enc, xtest_enc_1, xtest_enc_2, xtest_enc_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba9099c",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3. Data visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009f1f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def bar_plot(data, target_name):\n",
    "    matplotlib.pyplot.figure(figsize=(10,5))\n",
    "    ax = (data[target_name].value_counts()*100.0 /len(data[target_name])).plot(kind='bar', stacked = True, rot = 0)\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    ax.set_ylabel('Frequency Percentage')\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_title('Frequency Percentage by Class')\n",
    "    matplotlib.pyplot.grid(True)\n",
    "\n",
    "    totals = []  # list to collect the plt.patches data\n",
    "\n",
    "    # values and append to list\n",
    "    for i in ax.patches:\n",
    "        totals.append(i.get_width())\n",
    "\n",
    "    total = sum(totals)  # setting individual bar lables using above list\n",
    "\n",
    "    for i in ax.patches:\n",
    "        ax.text(i.get_x()+.15, i.get_height()-3.5, str(round((i.get_height()/total), 1))+'%', color='black', weight = 'bold')\n",
    "    return matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108b621",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def data_visualisation(dataset, data_name, a):\n",
    "    numeric_feature_names = []\n",
    "    category_feature_names = []\n",
    "    colNames = dataset.columns.values.tolist()\n",
    "    for colName in colNames:\n",
    "        if (dataset[colName].dtypes == 'int64' or dataset[colName].dtypes == 'float64'):\n",
    "            numeric_feature_names.append(colName)\n",
    "        else:\n",
    "            category_feature_names.append(colName)\n",
    "            \n",
    "    if data_name =='ssq':\n",
    "        category_feature_names = category_feature_names[0: len(category_feature_names)-2] #'fullkey_sub' & 'd_cdpo_sub' removed    \n",
    "    \n",
    "    if (a==1):\n",
    "        # USING pie plots\n",
    "        i=1\n",
    "        print(); print(\"Les variables categorielles : \", category_feature_names)\n",
    "        print(); print(\"Camembert de chacune des variables categorielles\", len(category_feature_names))\n",
    "        matplotlib.pyplot.figure(figsize=(20,55))\n",
    "        for col in category_feature_names:\n",
    "            matplotlib.pyplot.gcf().subplots_adjust(wspace = 0.5)\n",
    "            matplotlib.pyplot.subplot(11,4,i)\n",
    "            dataset.groupby(category_feature_names[i-1]).size().plot(kind='pie', autopct='%1.1f%%', textprops={'fontsize': 15})\n",
    "            matplotlib.pyplot.title(category_feature_names[i-1], size=18)\n",
    "            matplotlib.pyplot.ylabel(' ')\n",
    "            i += 1\n",
    "        matplotlib.pyplot.show()\n",
    "    \n",
    "    if (a==2):\n",
    "        # BOX plots USING box and whisker plots\n",
    "        i=1\n",
    "        print(); print(\"Les variables numériques : \", numeric_feature_names)\n",
    "        print(); print(\"Boîte à moustache de chacune des variables numériques\", len(numeric_feature_names))\n",
    "        matplotlib.pyplot.figure(figsize=(14,100))\n",
    "        for col in numeric_feature_names:\n",
    "            matplotlib.pyplot.gcf().subplots_adjust(wspace = 0.5, hspace = 0.4)\n",
    "            matplotlib.pyplot.subplot(28,4,i)\n",
    "            matplotlib.pyplot.axis('on')\n",
    "            matplotlib.pyplot.tick_params(axis='both', left=True, top=False, right=False, bottom=True)\n",
    "            matplotlib.pyplot.tick_params(axis=\"x\", labelsize=12)\n",
    "            dataset[col].plot(kind='box', subplots=True, sharex=False, sharey=False)\n",
    "            i += 1\n",
    "        matplotlib.pyplot.show()\n",
    "    \n",
    "    if (a==3):\n",
    "        # USING histograms\n",
    "        j=1\n",
    "        print(); print(\"Histogramme de chacune des variables numériques\", len(numeric_feature_names))\n",
    "        matplotlib.pyplot.figure(figsize=(18,100))\n",
    "        for col in numeric_feature_names:\n",
    "            matplotlib.pyplot.gcf().subplots_adjust(wspace = 0.5, hspace = 0.4)\n",
    "            matplotlib.pyplot.subplot(28,4,j)\n",
    "            matplotlib.pyplot.axis('on')\n",
    "            matplotlib.pyplot.tick_params(axis='both', left=True, top=False, right=False, bottom=True)\n",
    "            dataset[col].hist()\n",
    "            matplotlib.pyplot.title(numeric_feature_names[j-1], size=15)\n",
    "            j += 1\n",
    "        matplotlib.pyplot.show()\n",
    "    \n",
    "    if (a==4):\n",
    "        # correlation matrix\n",
    "        print(); print(\"Matrice de corrélation pour toutes les variables numériques\", len(numeric_feature_names))\n",
    "        fig = matplotlib.pyplot.figure(figsize=(10,8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(dataset[numeric_feature_names].corr(), vmin=-1, vmax=1, interpolation='none')\n",
    "        fig.colorbar(cax)\n",
    "        matplotlib.pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
