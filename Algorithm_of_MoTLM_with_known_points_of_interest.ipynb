{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824481ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28404bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import math\n",
    "import numpy\n",
    "from numpy import linalg as LA\n",
    "import pandas\n",
    "from copy import deepcopy\n",
    "from scipy import special\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error, r2_score\n",
    "\n",
    "import import_ipynb\n",
    "import data_analysis\n",
    "import utility_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc15a32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2. Binary linear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd6704-1da9-4d79-a1c6-f2fb5cd5762c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Definition of mixture models class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2bd7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pac_bayes_clf:\n",
    "    def __init__(self, learning_rate=0.01, lambda_reg=100, W_ini=None, w_ext_ini=None, K_ini=None, TAU_ini=None, K_p=None, TAU_p=None, max_iters=2000, class_weights=None, print_flag=False):\n",
    "        self.lr = learning_rate                                    # the step size at each iteration\n",
    "        self.Lambda = lambda_reg                                   # the regularization rate (lambda_reg should be higher than 1)\n",
    "        self.W_ini, self.w_ext_ini = W_ini, w_ext_ini              # initial mean vector weights Wi and w_ext of the hyperplanes Vi and v_ext\n",
    "        self.K_ini, self.TAU_ini = K_ini, TAU_ini                  # initial values of the shape and rate of locality parameter beta to find (beta = k/tau)\n",
    "        self.K_p, self.TAU_p = K_p, TAU_p                          # prior value of the shape and rate (gamma distribution parameter)\n",
    "        self.max_iters = max_iters                                 # maximum number of iterations before stopping independently of any early stopping\n",
    "        self.weights = class_weights                               # to take into account the skewed distribution of the classes (if necessary), it just a flag\n",
    "        self.print_flag = print_flag                               # flag to print some infos\n",
    "        self.W, self.MU, self.SIGMA = None, None, None             # model parameters to be find in n localities\n",
    "        self.w_ext, self.mu_ext, self.sigma_ext = None, None, None # model external parameters to be find out of n localities\n",
    "        self.K, self.TAU, self.BETA = None, None, None             # the locality parameters beta to find beta = (k, tau)\n",
    "        self.weights_vet = None                                    # to take into account the weights the target (if necessary)\n",
    "    \n",
    "    def fit(self, X, y, Mat_dist):\n",
    "        # data shape and number of vicinity points\n",
    "        n_samples, n_features = X.shape\n",
    "        n_points = Mat_dist.shape[0]\n",
    "\n",
    "        # compute class_weight\n",
    "        if self.weights is None:\n",
    "            self.weights_vet = numpy.ones((n_samples,1)) \n",
    "        else:\n",
    "            class_weights = compute_class_weight(class_weight='balanced', classes=numpy.unique(y), y=y)\n",
    "            self.weights_vet = numpy.where(y == numpy.unique(y)[0], class_weights[0], class_weights[1])\n",
    "\n",
    "        # initialize parameters\n",
    "        if self.W_ini is None:\n",
    "            self.W_ini, self.w_ext_ini = torch.randn(n_points, n_features), torch.randn(n_features)\n",
    "            self.K_ini, self.TAU_ini = 50 * torch.rand(n_points), 50 * torch.rand(n_points)\n",
    "            \n",
    "        self.W, self.MU, self.SIGMA = deepcopy(self.W_ini), torch.zeros(n_points), torch.ones(n_points)\n",
    "        self.w_ext, self.mu_ext, self.sigma_ext = deepcopy(self.w_ext_ini), torch.tensor([0.0]), torch.tensor([1.0])\n",
    "        self.K, self.TAU = deepcopy(self.K_ini), deepcopy(self.TAU_ini)\n",
    "\n",
    "        # concatenate (initialize parameters)\n",
    "        starting_point = ((self.W, self.MU, self.SIGMA, self.w_ext, self.mu_ext, self.sigma_ext), (torch.from_numpy(self.K_p), torch.from_numpy(self.TAU_p)), (self.K, self.TAU))\n",
    "        \n",
    "        # convert data to tensor\n",
    "        X_, y_, Mat_dist_, self.weights_vet = torch.from_numpy(X), torch.from_numpy(y), torch.from_numpy(Mat_dist), torch.from_numpy(self.weights_vet)\n",
    "\n",
    "        # minimized params\n",
    "        gaussian_params, gamma_params = utility_functions.minimize_binary_clf_gn(starting_point, X_, y_, Mat_dist_, self.lr, self.Lambda, self.max_iters, self.weights_vet, self.print_flag)\n",
    "\n",
    "        # get minimized params and convert initial params to numpy array\n",
    "        self.W, self.MU, self.SIGMA, self.w_ext, self.mu_ext, self.sigma_ext = gaussian_params\n",
    "        self.K, self.TAU = gamma_params\n",
    "        self.BETA = numpy.round(self.K / self.TAU, 6)\n",
    "            \n",
    "    def predict(self, X, Mat_dists):\n",
    "        n_points = Mat_dists.shape[0]\n",
    "\n",
    "        W_params = numpy.concatenate((self.W, self.w_ext.reshape(1, self.w_ext.size)), axis=0)\n",
    "        MU_params = numpy.concatenate((self.MU, numpy.array([self.mu_ext])), axis=0)\n",
    "        approx = (numpy.matmul(X, W_params.T) + MU_params).T\n",
    "        \n",
    "        # get idx_W for each samples\n",
    "        idx_W = numpy.argmin((Mat_dists + numpy.where(Mat_dists <= self.BETA.reshape(n_points,1), 0, 1)), axis=0)\n",
    "        number_of_overlap_region = numpy.sum(numpy.where(Mat_dists <= self.BETA.reshape(n_points,1), 1, 0), axis=0)\n",
    "        idx_W[numpy.where(number_of_overlap_region == 0)[0]] = n_points\n",
    "\n",
    "        mask = numpy.nonzero( (numpy.ones((n_points+1,1)) * idx_W) == (numpy.arange(n_points+1)).reshape(n_points+1,1) )\n",
    "        mask = (mask[0][numpy.argsort(mask[1])], numpy.sort(mask[1]))\n",
    "        return numpy.where(numpy.sign(approx[mask]) <= 0, -1, 1)\n",
    "    \n",
    "    def score(self, X, y, Mat_dists):\n",
    "        return numpy.round(accuracy_score(y, self.predict(X, Mat_dists))*100, 4)\n",
    "\n",
    "    def risk_bound(self, X, y, Mat_dists):\n",
    "        # get data shape\n",
    "        n_points, n_samples = Mat_dists.shape\n",
    "        \n",
    "        # compute Theta\n",
    "        Mat_Theta = special.gdtrc(self.TAU.reshape(n_points,1), self.K.reshape(n_points, 1), Mat_dists)\n",
    "        \n",
    "        # Mat_loss_in and Loss_out\n",
    "        norm_X_square = LA.norm(X, axis=1)**2\n",
    "        Mat_loss_in = 1 - special.ndtr( (y.reshape(n_samples, 1) * (numpy.dot(X, self.W.T) + self.MU)) / numpy.sqrt(self.SIGMA**2 + norm_X_square.reshape(n_samples, 1)) ).T\n",
    "        Loss_out = 1 - special.ndtr( (y * (numpy.dot(X, self.w_ext) + self.mu_ext)) / numpy.sqrt(self.sigma_ext**2 + norm_X_square) )\n",
    "    \n",
    "        # compute empirical risk\n",
    "        ER = numpy.mean(numpy.sum(Mat_Theta * Mat_loss_in, axis=0) + numpy.prod(1 - Mat_Theta, axis=0) * Loss_out)\n",
    "        \n",
    "        # compute KL divergence\n",
    "        gaussian_kl_var = - math.log(numpy.prod(self.SIGMA) * self.sigma_ext) + 0.5 * (numpy.sum(self.SIGMA**2) + self.sigma_ext**2 - (n_points + 1))\n",
    "        gaussian_kl = 0.5 * (numpy.sum(LA.norm(self.W, axis=1)**2, axis=0) + numpy.sum(self.MU**2, axis=0) + LA.norm(self.w_ext)**2 + self.mu_ext**2)\n",
    "        gamma_kl = numpy.sum( (self.K - self.K_p) * special.psi(self.K) + special.gammaln(self.K_p) - special.gammaln(self.K) + self.K_p * numpy.log(self.TAU / self.TAU_p) + self.K * ((self.TAU_p - self.TAU) / self.TAU) )\n",
    "        return ER, ER + (1 / self.Lambda) * (gaussian_kl_var + gaussian_kl + gamma_kl)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386b4b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Choice of Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de5098-7cf8-4b07-b715-f47803cd59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mixture_clf(X_train, X_val, y_train, y_val, Mat_dist_train, Mat_dist_val, T=10, lr=0.01, lambda_param=1000, K_p=None, TAU_p=None, max_iters=2000, class_weights=None, print_flag=False): \n",
    "    old_val_ER, old_val_risk = 1e10, 1e10\n",
    "    for i in range(T):\n",
    "        mixture_clf = Pac_bayes_clf(learning_rate=lr, lambda_reg=lambda_param, K_p=K_p, TAU_p=TAU_p, max_iters=max_iters, class_weights=class_weights, print_flag=print_flag)\n",
    "        mixture_clf.fit(X_train, y_train, Mat_dist_train)\n",
    "        new_val_ER, new_val_risk = mixture_clf.risk_bound(X_val, y_val, Mat_dist_val)\n",
    "        \n",
    "        if (new_val_ER < old_val_ER) and (new_val_risk < old_val_risk):\n",
    "            Initial_parameters = (mixture_clf.W_ini, mixture_clf.w_ext_ini, mixture_clf.K_ini, mixture_clf.TAU_ini)\n",
    "            val_score, old_val_ER, old_val_risk = mixture_clf.score(X_val, y_val, Mat_dist_val), new_val_ER, new_val_risk\n",
    "    return val_score, Initial_parameters\n",
    "\n",
    "def lambda_validation_clf(X_train, X_val, y_train, y_val, Mat_dist_train, Mat_dist_val, lr=0.01, K_p=None, TAU_p=None, max_iters=2000, class_weights=None): \n",
    "    T, old_val_score = 10 * len(K_p), 0\n",
    "    lambda_params = numpy.arange(1, 5.25, 0.25) * (1 / len(K_p)) * X_train.shape[0]\n",
    "\n",
    "    i = 0\n",
    "    pbar = tqdm(desc=\"Tuning Lambda (\"+str(T)+\" random restarts for each lambda) : \", total=len(lambda_params), position=0)\n",
    "        \n",
    "    while(i < len(lambda_params)) :\n",
    "        lambda_param = lambda_params[i]\n",
    "        torch.manual_seed(lambda_param)\n",
    "        new_val_score, new_Initial_parameters = build_mixture_clf(X_train, X_val, y_train, y_val, Mat_dist_train, Mat_dist_val, T=T, lr=lr, lambda_param=lambda_param, \n",
    "                                                                  K_p=K_p, TAU_p=TAU_p, max_iters=max_iters, class_weights=class_weights, print_flag=False)\n",
    "        if (new_val_score >= old_val_score) :\n",
    "            lambda_param_, Initial_parameters, old_val_score = lambda_param, new_Initial_parameters, new_val_score\n",
    "        \n",
    "        pbar.update(1)\n",
    "        i = i + 1\n",
    "\n",
    "        elapsed = pbar.format_dict[\"elapsed\"]\n",
    "        if elapsed > 1800:\n",
    "            break;\n",
    "            \n",
    "    pbar.close()\n",
    "    return lambda_param_, Initial_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fcf46d-aa2c-49e5-ba42-c2a7a06a3b93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Mixture Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixture_clf(data, target_name, X0, synthetic_data_flag=False, train_size=0.75, lr=0.01, lambda_param=100, max_iters=2000, lambda_validation=False, times=1, return_flag='simple'):\n",
    "    # given point of interest X0 (center of vicinity data or specific point)\n",
    "    assert isinstance(X0, pandas.DataFrame)\n",
    "    \n",
    "    # uncouping X and y\n",
    "    X, y = data_analysis.uncouping_x_y_clf(data.copy(), target_name)\n",
    "    class_weights = None\n",
    "    print('***************** Mixtures of transparent local models with known points of interest *****************')\n",
    "    print(f'Training_set = {round((train_size * 100))}%, Validation_set = {round(((1 - train_size)/2) * 100)}%, Test_set = {round(((1 - train_size)/2) * 100)}%, class_weights = {class_weights}, lambda_validation = {lambda_validation}, times = {times}')\n",
    "    \n",
    "    All_acc_score = []\n",
    "    for i in tqdm(numpy.arange(times), desc=\"For Random Data Split = \"+str(times)+\" …\", total=times, position=0):\n",
    "        # split the dataset X into the training set X_train and temporary set X_temp\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size = train_size, stratify=y, random_state=i)\n",
    "        # split the dataset X_temp into the validation set X_val and testing set X_test\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size = 0.5, stratify=y_temp, random_state=0)\n",
    "        X_train, X_val, X_test = data_analysis.reset_index_data(data_1=X_train, data_2=X_val, data_3=X_test, data_4=None)\n",
    "    \n",
    "        # define gamma_priors\n",
    "        K_p, TAU_p = 2.0 * numpy.ones(X0.shape[0]), (1 / 10) * numpy.ones(X0.shape[0])\n",
    "        \n",
    "        # data encoding (target encoding for category variables) and scaling (example : 'TargetEncoder', 'OrdinalEncoder', etc...)\n",
    "        if synthetic_data_flag==False:\n",
    "            X_train_enc, X_val_enc, X_test_enc, X0_enc = data_analysis.data_processing(xtrain=X_train.copy(), ytrain=y_train.copy(), xtest_1=X_val.copy(), xtest_2=X_test.copy(), xtest_3=X0.copy(), check_multicollinearity=True)\n",
    "            X_train_enc, X_val_enc, X_test_enc, X0_enc = X_train_enc.values.copy(), X_val_enc.values.copy(), X_test_enc.values.copy(), X0.values.copy()\n",
    "        else:\n",
    "            X_train_enc, X_val_enc, X_test_enc, X0_enc = X_train.values.copy(), X_val.values.copy(), X_test.values.copy(), X0.values.copy()\n",
    "\n",
    "         # get set of vicinity distance (example : 'euclidean', 'gower', etc...)\n",
    "        Mat_dist_train = LA.norm(X_train_enc - X0_enc.reshape(X0_enc.shape[0], 1, X0_enc.shape[1]), axis=2)\n",
    "        Mat_dist_val = LA.norm(X_val_enc - X0_enc.reshape(X0_enc.shape[0], 1, X0_enc.shape[1]), axis=2)\n",
    "        Mat_dist_test = LA.norm(X_test_enc - X0_enc.reshape(X0_enc.shape[0], 1, X0_enc.shape[1]), axis=2)\n",
    "\n",
    "        # finding the best lambda by cross validation on data\n",
    "        if lambda_validation == True:\n",
    "            lambda_param, Initial_parameters = lambda_validation_clf(X_train_enc.copy(), X_val_enc.copy(), y_train.copy(), y_val.copy(), Mat_dist_train, Mat_dist_val, lr=lr, K_p=K_p, TAU_p=TAU_p, max_iters=max_iters, class_weights=class_weights)\n",
    "        else :\n",
    "            torch.manual_seed(lambda_param)\n",
    "            Acc_score, Initial_parameters = build_mixture_clf(X_train_enc.copy(), X_val_enc.copy(), y_train.copy(), y_val.copy(), Mat_dist_train, Mat_dist_val, T=50, lr=lr, lambda_param=lambda_param, K_p=K_p, TAU_p=TAU_p, max_iters=max_iters, class_weights=class_weights, print_flag=False)\n",
    "        \n",
    "        # fitting model\n",
    "        mixture_clf = Pac_bayes_clf(learning_rate=lr, lambda_reg=lambda_param, W_ini=Initial_parameters[0], w_ext_ini=Initial_parameters[1], K_ini=Initial_parameters[2], TAU_ini=Initial_parameters[3], K_p=K_p.copy(), TAU_p=TAU_p.copy(), max_iters=max_iters, class_weights=class_weights, print_flag=False)\n",
    "        mixture_clf.fit(X_train_enc, y_train, Mat_dist_train)\n",
    "        \n",
    "        # prediction\n",
    "        y_train_preds = mixture_clf.predict(X_train_enc, Mat_dist_train)\n",
    "        y_val_preds = mixture_clf.predict(X_val_enc, Mat_dist_val)\n",
    "        y_test_preds = mixture_clf.predict(X_test_enc, Mat_dist_test)\n",
    "        \n",
    "        # compute risk for each dataset\n",
    "        risk_bound_set = []\n",
    "        risk_bound_set.append(mixture_clf.risk_bound(X_train_enc, y_train, Mat_dist_train))\n",
    "        risk_bound_set.append(mixture_clf.risk_bound(X_val_enc, y_val, Mat_dist_val))\n",
    "        risk_bound_set.append(mixture_clf.risk_bound(X_test_enc, y_test, Mat_dist_test))\n",
    "        risk_bound_set = numpy.round(numpy.array(risk_bound_set), 4)\n",
    "    \n",
    "        # get summary\n",
    "        summary_random = results_summary_clf(y_train, y_val, y_test, y_train_preds, y_val_preds, y_test_preds, risk_bound_set[:,0], risk_bound_set[:,1])\n",
    "        All_acc_score.append(summary_random['Accuracy'].values)\n",
    "\n",
    "        if (i == 0):\n",
    "            W_random_state, MU_random_state, SIGMA_random_state = mixture_clf.W, mixture_clf.MU, mixture_clf.SIGMA\n",
    "            w_ext_random_state, mu_ext_random_state, sigma_ext_random_state = mixture_clf.w_ext, mixture_clf.mu_ext, mixture_clf.sigma_ext\n",
    "            K_random_state, TAU_random_state, BETA_random_state = mixture_clf.K, mixture_clf.TAU, mixture_clf.BETA\n",
    "            summary_random_state = summary_random\n",
    "        else:\n",
    "            W_random_state += mixture_clf.W\n",
    "            MU_random_state += mixture_clf.MU\n",
    "            SIGMA_random_state += mixture_clf.SIGMA\n",
    "            w_ext_random_state += mixture_clf.w_ext\n",
    "            mu_ext_random_state += mixture_clf.mu_ext\n",
    "            sigma_ext_random_state += mixture_clf.sigma_ext\n",
    "            K_random_state += mixture_clf.K\n",
    "            TAU_random_state += mixture_clf.TAU\n",
    "            BETA_random_state += mixture_clf.BETA\n",
    "            summary_random_state += summary_random\n",
    "        \n",
    "        print(f'W = {mixture_clf.W}, MU = {numpy.round(mixture_clf.MU, 6)}, lambda_param = {lambda_param}')\n",
    "        print(f'w_ext = {mixture_clf.w_ext}, mu_ext = {round(mixture_clf.mu_ext, 6)}')\n",
    "        print(f'SIGMA = {numpy.round(mixture_clf.SIGMA, 6)}, sigma_ext = {round(mixture_clf.sigma_ext, 6)}')\n",
    "\n",
    "    W, MU, SIGMA = (W_random_state / times), (MU_random_state / times), (SIGMA_random_state / times)\n",
    "    w_ext, mu_ext, sigma_ext = (w_ext_random_state / times), (mu_ext_random_state / times), (sigma_ext_random_state / times)\n",
    "    K, TAU, BETA = (K_random_state / times), (TAU_random_state / times), (BETA_random_state / times)\n",
    "    summary = (summary_random_state / times).astype('float64')\n",
    "    summary['Std_accuracy'] = numpy.std(numpy.array(All_acc_score, dtype='float64'), axis=0) # add column for accuracy standard deviation\n",
    "    summary = summary.round(4)\n",
    "    \n",
    "    if return_flag=='simple':\n",
    "        return X0_enc, lambda_param, W, w_ext, MU, mu_ext, K, TAU, BETA, summary\n",
    "    else :\n",
    "        return X0_enc, lambda_param, W, w_ext, MU, mu_ext, K, TAU, BETA, summary, X_train_enc, X_test_enc, Mat_dist_train, Mat_dist_test, y_train.copy(), y_test.copy(), y_train_preds.copy(), y_test_preds.copy()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba9ac9",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Performance measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf379078",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def results_summary_clf(ytrain_true, yval_true, ytest_true, ytrain_pred, yval_pred, ytest_pred, Gibbs_risk_set, risk_bound_set):\n",
    "    # for global model\n",
    "    Summary_index = ['Training set', 'Validation set', 'Testing set']\n",
    "    Summary_columns = ['Accuracy', 'F1_score', 'Precision_score', 'Gibbs_risk', 'Risk_bound']\n",
    "    Summary_results = pandas.DataFrame(index=Summary_index, columns=Summary_columns)\n",
    "\n",
    "    # performance of the global model\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[0]] = round(accuracy_score(ytrain_true, ytrain_pred)*100, 2)\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[1]] = round(f1_score(ytrain_true, ytrain_pred)*100, 2)\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[2]] = round(average_precision_score(ytrain_true, ytrain_pred)*100, 2)\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[3]] = Gibbs_risk_set[0]\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[4]] = risk_bound_set[0]\n",
    "    \n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[0]] = round(accuracy_score(yval_true, yval_pred)*100, 2)\n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[1]] = round(f1_score(yval_true, yval_pred)*100, 2)\n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[2]] = round(average_precision_score(yval_true, yval_pred)*100, 2)\n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[3]] = Gibbs_risk_set[1]\n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[4]] = risk_bound_set[1]\n",
    "    \n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[0]] = round(accuracy_score(ytest_true, ytest_pred)*100, 2)\n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[1]] = round(f1_score(ytest_true, ytest_pred)*100, 2)\n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[2]] = round(average_precision_score(ytest_true, ytest_pred)*100, 2)\n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[3]] = Gibbs_risk_set[2]\n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[4]] = risk_bound_set[2]\n",
    "    \n",
    "    return Summary_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bc18e-252f-41c6-8a76-f6148fecf55a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9b29d-4503-4151-93a1-82177c6422b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Definition of mixture models class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a3793-f833-43dd-9e6a-8b4e2bf55250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pac_bayes_reg:\n",
    "    def __init__(self, learning_rate=3, lambda_reg=0.1, K_ini=None, TAU_ini=None, K_p=None, TAU_p=None, max_iters=1000, weights=None, print_flag=False):\n",
    "        self.lr = learning_rate                                    # the step size at each iteration\n",
    "        self.Lambda = lambda_reg                                   # the regularization rate (lambda_reg should be higher than 1)\n",
    "        self.K_ini, self.TAU_ini = K_ini, TAU_ini                  # initial values of the shape and rate of locality parameter beta to find (beta = k/tau)\n",
    "        self.K_p, self.TAU_p = K_p, TAU_p                          # prior value of the shape and rate (gamma distribution parameter)\n",
    "        self.max_iters = max_iters                                 # maximum number of iterations before stopping independently of any early stopping\n",
    "        self.weights_vet = weights                                 # to take into account the skewed distribution of the classes (if necessary)\n",
    "        self.print_flag = print_flag                               # flag to print some infos\n",
    "        self.W, self.RHO, self.MU, self.SIGMA = None, None, None, None                 # model parameters to be find in n localities\n",
    "        self.w_ext, self.rho_ext, self.mu_ext, self.sigma_ext = None, None, None, None # model external parameters to be find out of n localities\n",
    "        self.K, self.TAU, self.BETA = None, None, None                                 # the locality parameters beta to find beta = (k, tau)\n",
    "        self.y_std = None\n",
    "\n",
    "    def fit(self, X, y, Mat_dist):\n",
    "        # data shape and number of vicinity points\n",
    "        n_samples, n_features = X.shape\n",
    "        n_points = Mat_dist.shape[0]\n",
    "        self.y_std = numpy.std(y)\n",
    "\n",
    "        # compute weight\n",
    "        self.weights_vet = numpy.ones(n_samples) if self.weights_vet is None else self.weights_vet\n",
    "\n",
    "        # initialize parameters\n",
    "        if self.K_ini is None:\n",
    "            self.K_ini, self.TAU_ini = 50 * torch.rand(n_points), 50 * torch.rand(n_points)\n",
    "\n",
    "        self.K, self.TAU = deepcopy(self.K_ini), deepcopy(self.TAU_ini)\n",
    "        \n",
    "        self.W, self.RHO = torch.zeros(n_points, n_features), torch.ones(n_points)\n",
    "        self.MU, self.SIGMA = torch.zeros(n_points), torch.ones(n_points)\n",
    "\n",
    "        self.w_ext, self.rho_ext = torch.zeros(n_features), torch.tensor([1.0])\n",
    "        self.mu_ext, self.sigma_ext = torch.tensor([0.0]), torch.tensor([1.0])\n",
    "\n",
    "        # concatenate (initialize parameters)\n",
    "        starting_point = ((self.W, self.RHO, self.MU, self.SIGMA, self.w_ext, self.rho_ext, self.mu_ext, self.sigma_ext), (torch.from_numpy(self.K_p), torch.from_numpy(self.TAU_p)), (self.K, self.TAU))\n",
    "        \n",
    "        # convert data to tensor\n",
    "        X_, y_, Mat_dist_, self.weights_vet = torch.from_numpy(X), torch.from_numpy(y / self.y_std), torch.from_numpy(Mat_dist), torch.from_numpy(self.weights_vet)\n",
    "\n",
    "        # minimized params\n",
    "        gaussian_params, gamma_params = utility_functions.minimize_reg_gn(starting_point, X_, y_, Mat_dist_, self.lr, self.Lambda, self.max_iters, self.weights_vet, self.print_flag)\n",
    "\n",
    "        # get minimized params and convert initial params to numpy array\n",
    "        self.W, self.RHO, self.MU, self.SIGMA, self.w_ext, self.rho_ext, self.mu_ext, self.sigma_ext = gaussian_params\n",
    "        self.K, self.TAU = gamma_params\n",
    "        self.BETA = numpy.round(self.K / self.TAU, 6)\n",
    "            \n",
    "    def predict(self, X, Mat_dists):\n",
    "        n_points = Mat_dists.shape[0]\n",
    "\n",
    "        W_params = numpy.concatenate((self.W, self.w_ext.reshape(1, self.w_ext.size)), axis=0)\n",
    "        MU_params = numpy.concatenate((self.MU, numpy.array([self.mu_ext])), axis=0)\n",
    "        approx = ( (numpy.matmul(X, W_params.T) + MU_params) * self.y_std ).T\n",
    "        \n",
    "        # get idx_W for each samples\n",
    "        idx_W = numpy.argmin((Mat_dists + numpy.where(Mat_dists <= self.BETA.reshape(n_points,1), 0, 1)), axis=0)\n",
    "        number_of_overlap_region = numpy.sum(numpy.where(Mat_dists <= self.BETA.reshape(n_points,1), 1, 0), axis=0)\n",
    "        idx_W[numpy.where(number_of_overlap_region == 0)[0]] = n_points\n",
    "\n",
    "        mask = numpy.nonzero( (numpy.ones((n_points+1,1)) * idx_W) == (numpy.arange(n_points+1)).reshape(n_points+1,1) )\n",
    "        mask = (mask[0][numpy.argsort(mask[1])], numpy.sort(mask[1]))\n",
    "        return approx[mask]\n",
    "    \n",
    "    def score(self, X, y, Mat_dists):\n",
    "        return numpy.round(r2_score(y_true=y, y_pred=self.predict(X, Mat_dists)), 6)\n",
    "\n",
    "    def risk_bound(self, X, y, Mat_dists):\n",
    "        n_points, n_samples = Mat_dists.shape\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # compute Theta\n",
    "        Mat_Theta = special.gdtrc(self.TAU.reshape(n_points,1), self.K.reshape(n_points, 1), Mat_dists)\n",
    "        \n",
    "        # Mat_loss_in and Loss_out\n",
    "        norm_X_square = LA.norm(X, axis=1)**2\n",
    "        Mat_loss_in = (norm_X_square.reshape(n_samples, 1) * self.RHO**2 + self.SIGMA**2 + ((numpy.dot(X, self.W.T) + self.MU) * self.y_std - y.reshape(n_samples, 1))**2).T\n",
    "        Loss_out = norm_X_square * self.rho_ext**2 + self.sigma_ext**2 + ((numpy.dot(X, self.w_ext) + self.mu_ext) * self.y_std - y)**2\n",
    "\n",
    "        # compute empirical risk\n",
    "        ER = numpy.mean(numpy.sum(Mat_Theta * Mat_loss_in, axis=0) + numpy.prod(1 - Mat_Theta, axis=0) * Loss_out)\n",
    "        \n",
    "        # compute KL divergence\n",
    "        gaussian_kl = - n_features * math.log(numpy.prod(self.RHO) * self.rho_ext) - math.log(numpy.prod(self.SIGMA) * self.sigma_ext) + 0.5 * (n_features * (numpy.sum(self.RHO**2) + self.rho_ext**2) + (numpy.sum(self.SIGMA**2) + self.sigma_ext**2) - ((n_points + 1) * n_features + (n_points + 1)))\n",
    "        gaussian_kl += 0.5 * (numpy.sum(LA.norm(self.W, axis=1)**2, axis=0) + numpy.sum(self.MU**2, axis=0) + LA.norm(self.w_ext)**2 + self.mu_ext**2)\n",
    "        gamma_kl = numpy.sum((self.K - self.K_p) * special.psi(self.K) + special.gammaln(self.K_p) - special.gammaln(self.K) + self.K_p * numpy.log(self.TAU / self.TAU_p) + self.K * ((self.TAU_p - self.TAU) / self.TAU))\n",
    "        return ER, ER + (1 / self.Lambda) * (gaussian_kl + gamma_kl)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ebb96-ea8f-466d-b600-a25eb4fcaa5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Choice of Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b20dd-2af6-4aef-8e01-ce5a6ffd07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mixture_reg(X_train, X_val, y_train, y_val, Mat_dist_train, Mat_dist_val, T=10, lr=0.01, lambda_param=1000, K_p=None, TAU_p=None, max_iters=1000, weights=None, print_flag=False): \n",
    "    old_val_ER, old_val_risk = 1e10, 1e10\n",
    "    for i in range(T):\n",
    "        mixture_reg = Pac_bayes_reg(learning_rate=lr, lambda_reg=lambda_param, K_p=K_p, TAU_p=TAU_p, max_iters=max_iters, weights=weights, print_flag=print_flag)\n",
    "        mixture_reg.fit(X_train, y_train, Mat_dist_train)\n",
    "        new_val_ER, new_val_risk = mixture_reg.risk_bound(X_val, y_val, Mat_dist_val)\n",
    "\n",
    "        if (new_val_ER < old_val_ER) and (new_val_risk < old_val_risk):\n",
    "            Initial_parameters = (mixture_reg.K_ini, mixture_reg.TAU_ini)\n",
    "            val_score, old_val_ER, old_val_risk = mixture_reg.score(X_val, y_val, Mat_dist_val), new_val_ER, new_val_risk\n",
    "    return val_score, Initial_parameters\n",
    "\n",
    "def lambda_validation_reg(X_train, X_val, y_train, y_val, Mat_dist_train, Mat_dist_val, lr=0.01, K_p=None, TAU_p=None, max_iters=1000, weights=None): \n",
    "    T, old_val_score = 10 * len(K_p), -1e10\n",
    "    lambda_params = numpy.arange(1, 5.25, 0.25) * (1 / len(K_p)) * X_train.shape[0]\n",
    "\n",
    "    i = 0\n",
    "    pbar = tqdm(desc=\"Tuning Lambda (\"+str(T)+\" random restarts for each lambda) : \", total=len(lambda_params), position=0)\n",
    "        \n",
    "    while(i < len(lambda_params)) :\n",
    "        lambda_param = lambda_params[i]\n",
    "        torch.manual_seed(lambda_param)\n",
    "        new_val_score, new_Initial_parameters = build_mixture_reg(X_train, X_val, y_train, y_val, Mat_dist_train, Mat_dist_val, T=T, lr=lr, lambda_param=lambda_param, \n",
    "                                                                  K_p=K_p, TAU_p=TAU_p, max_iters=max_iters, weights=weights, print_flag=False)\n",
    "        if (new_val_score >= old_val_score):\n",
    "            lambda_param_, Initial_parameters, old_val_score = lambda_param, new_Initial_parameters, new_val_score\n",
    "        \n",
    "        pbar.update(1)\n",
    "        i = i + 1\n",
    "\n",
    "        elapsed = pbar.format_dict[\"elapsed\"]\n",
    "        if elapsed > 1800:\n",
    "            break;\n",
    "            \n",
    "    pbar.close()\n",
    "    return lambda_param_, Initial_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d24b9f-0568-4f79-ac29-07d5db9365be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Mixture Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d928e-e95c-43a6-91b7-face77810fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixture_reg(data, target_name, X0, synthetic_data_flag=False, train_size=0.75, lr=0.01, lambda_param=1e-3, max_iters=1000, lambda_validation=False, times=1, return_flag='simple'):\n",
    "    # given point of interest X0 (center of vicinity data or specific point)\n",
    "    assert isinstance(X0, pandas.DataFrame)\n",
    "\n",
    "    # uncouping X and y reg\n",
    "    X, y = data_analysis.uncouping_x_y_reg(data, target_name)\n",
    "    weights = None\n",
    "    print('***************** Mixtures of transparent local models with known points of interest *****************')\n",
    "    print(f'Training_set = {round((train_size * 100))}%, Validation_set = {round(((1 - train_size)/2) * 100)}%, Test_set = {round(((1 - train_size)/2) * 100)}%, weights = {weights}, lambda_validation = {lambda_validation}, times = {times}')\n",
    "\n",
    "    for i in tqdm(numpy.arange(times), desc=\"For Random Data Split = \"+str(times)+\" …\", total=times, position=0):\n",
    "        # split the dataset X into the training set X_train and temporary set X_temp\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size = train_size, random_state=i)\n",
    "        # split the dataset X_temp into the validation set X_val and testing set X_test\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size = 0.5, random_state=0)\n",
    "        X_train, X_val, X_test = data_analysis.reset_index_data(data_1=X_train, data_2=X_val, data_3=X_test, data_4=None)\n",
    "    \n",
    "        # define gamma_priors\n",
    "        K_p, TAU_p = 2.0 * numpy.ones(X0.shape[0]), (1 / 10) * numpy.ones(X0.shape[0])\n",
    "        \n",
    "        # data encoding (target encoding for category variables) and scaling (example : 'TargetEncoder', 'OrdinalEncoder', etc...)\n",
    "        if synthetic_data_flag==False:\n",
    "            X_train_enc, X_val_enc, X_test_enc, X0_enc = data_analysis.data_processing(xtrain=X_train.copy(), ytrain=y_train.copy(), xtest_1=X_val.copy(), xtest_2=X_test.copy(), xtest_3=X0.copy(), check_multicollinearity=True)\n",
    "            X_train_enc, X_val_enc, X_test_enc, X0_enc = X_train_enc.values.copy(), X_val_enc.values.copy(), X_test_enc.values.copy(), X0.values.copy()\n",
    "        else:\n",
    "            X_train_enc, X_val_enc, X_test_enc, X0_enc = X_train.values.copy(), X_val.values.copy(), X_test.values.copy(), X0.values.copy()\n",
    "\n",
    "        # get set of vicinity distance (example : 'euclidean', 'gower', etc...)\n",
    "        Mat_dist_train = LA.norm(X_train_enc - X0_enc.reshape(X0_enc.shape[0], 1, X0_enc.shape[1]), axis=2)\n",
    "        Mat_dist_val = LA.norm(X_val_enc - X0_enc.reshape(X0_enc.shape[0], 1, X0_enc.shape[1]), axis=2)\n",
    "        Mat_dist_test = LA.norm(X_test_enc - X0_enc.reshape(X0_enc.shape[0], 1, X0_enc.shape[1]), axis=2)        \n",
    "        \n",
    "        # finding the best lambda by cross validation on data\n",
    "        if lambda_validation == True:\n",
    "            lambda_param, Initial_parameters = lambda_validation_reg(X_train_enc.copy(), X_val_enc.copy(), y_train.copy(), y_val.copy(), Mat_dist_train, Mat_dist_val, lr=lr, K_p=K_p, TAU_p=TAU_p, max_iters=max_iters, weights=weights)\n",
    "        else :\n",
    "            torch.manual_seed(lambda_param)\n",
    "            R2_score, Initial_parameters = build_mixture_reg(X_train_enc.copy(), X_val_enc.copy(), y_train.copy(), y_val.copy(), Mat_dist_train, Mat_dist_val, T=50, lr=lr, lambda_param=lambda_param, K_p=K_p, TAU_p=TAU_p, max_iters=max_iters, weights=weights, print_flag=False)\n",
    "            \n",
    "        # fitting model\n",
    "        mixture_reg = Pac_bayes_reg(learning_rate=lr, lambda_reg=lambda_param, K_ini=Initial_parameters[0], TAU_ini=Initial_parameters[1], K_p=K_p.copy(), TAU_p=TAU_p.copy(), max_iters=max_iters, weights=weights, print_flag=False)\n",
    "        mixture_reg.fit(X_train_enc, y_train, Mat_dist_train)\n",
    "        \n",
    "        # prediction\n",
    "        y_train_preds = mixture_reg.predict(X_train_enc, Mat_dist_train)\n",
    "        y_val_preds = mixture_reg.predict(X_val_enc, Mat_dist_val)\n",
    "        y_test_preds = mixture_reg.predict(X_test_enc, Mat_dist_test)\n",
    "        \n",
    "        # compute risk for each dataset\n",
    "        risk_bound_set = []\n",
    "        risk_bound_set.append(mixture_reg.risk_bound(X_train_enc, y_train, Mat_dist_train))\n",
    "        risk_bound_set.append(mixture_reg.risk_bound(X_val_enc, y_val, Mat_dist_val))\n",
    "        risk_bound_set.append(mixture_reg.risk_bound(X_test_enc, y_test, Mat_dist_test))\n",
    "        risk_bound_set = numpy.round(numpy.array(risk_bound_set), 4)\n",
    "    \n",
    "        # get summary\n",
    "        summary_random = results_summary_reg(y_train, y_val, y_test, y_train_preds, y_val_preds, y_test_preds, risk_bound_set[:,0], risk_bound_set[:,1])\n",
    "\n",
    "        if (i == 0):\n",
    "            W_random_state, RHO_random_state, MU_random_state, SIGMA_random_state = mixture_reg.W, mixture_reg.RHO, mixture_reg.MU, mixture_reg.SIGMA\n",
    "            w_ext_random_state, rho_ext_random_state, mu_ext_random_state, sigma_ext_random_state = mixture_reg.w_ext, mixture_reg.rho_ext, mixture_reg.mu_ext, mixture_reg.sigma_ext\n",
    "            K_random_state, TAU_random_state, BETA_random_state = mixture_reg.K, mixture_reg.TAU, mixture_reg.BETA\n",
    "            summary_random_state = summary_random\n",
    "        else:\n",
    "            W_random_state += mixture_reg.W\n",
    "            RHO_random_state += mixture_reg.RHO\n",
    "            MU_random_state += mixture_reg.MU\n",
    "            SIGMA_random_state += mixture_reg.SIGMA\n",
    "            w_ext_random_state += mixture_reg.w_ext\n",
    "            rho_ext_random_state += mixture_reg.rho_ext\n",
    "            mu_ext_random_state += mixture_reg.mu_ext\n",
    "            sigma_ext_random_state += mixture_reg.sigma_ext\n",
    "            K_random_state += mixture_reg.K\n",
    "            TAU_random_state += mixture_reg.TAU\n",
    "            BETA_random_state += mixture_reg.BETA\n",
    "            summary_random_state += summary_random\n",
    "            \n",
    "        print(f'W = {mixture_reg.W}, MU = {numpy.round(mixture_reg.MU, 6)}, lambda_param = {lambda_param}')\n",
    "        print(f'w_ext = {mixture_reg.w_ext}, mu_ext = {round(mixture_reg.mu_ext, 6)}')\n",
    "        print(f'RHO = {mixture_reg.RHO}, SIGMA = {numpy.round(mixture_reg.SIGMA, 6)}')\n",
    "        print(f'rho_ext = {mixture_reg.rho_ext}, sigma_ext = {round(mixture_reg.sigma_ext, 6)}')\n",
    "\n",
    "    W, RHO, MU, SIGMA = (W_random_state / times), (RHO_random_state / times), (MU_random_state / times), (SIGMA_random_state / times)\n",
    "    w_ext, rho_ext, mu_ext, sigma_ext = (w_ext_random_state / times), (rho_ext_random_state / times), (mu_ext_random_state / times), (sigma_ext_random_state / times)\n",
    "    K, TAU, BETA = (K_random_state / times), (TAU_random_state / times), (BETA_random_state / times)\n",
    "    summary = (summary_random_state / times).astype('float64')\n",
    "    summary = summary.round(4)\n",
    "\n",
    "    print(f'*********** END ***********')\n",
    "    print(f'W = {W}, MU = {numpy.round(MU, 6)}')\n",
    "    print(f'w_ext = {w_ext}, mu_ext = {round(mu_ext, 6)}')\n",
    "    print(f'RHO = {RHO}, SIGMA = {numpy.round(SIGMA, 6)}')\n",
    "    print(f'rho_ext = {rho_ext}, sigma_ext = {round(sigma_ext, 6)}')\n",
    "    \n",
    "    if return_flag=='simple':\n",
    "        return X0_enc, lambda_param, W, w_ext, MU, mu_ext, K, TAU, BETA, summary\n",
    "    else :\n",
    "        return X0_enc, lambda_param, W, w_ext, MU, mu_ext, K, TAU, BETA, summary, X_train_enc, X_test_enc, Mat_dist_train, Mat_dist_test, y_train.copy(), y_test.copy(), y_train_preds.copy(), y_test_preds.copy()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae4c0b-1674-4e82-8775-72377134e625",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Performance measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c6ec5-950c-4cd8-9bf6-112fa2b666c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_summary_reg(ytrain_true, yval_true, ytest_true, ytrain_pred, yval_pred, ytest_pred, Gibbs_risk_set, risk_bound_set):\n",
    "    # for global model\n",
    "    Summary_index = ['Training set', 'Validation set', 'Testing set']\n",
    "    Summary_columns = ['R2_score', 'RMSE', 'MSE', 'Gibbs_risk', 'Risk_bound']\n",
    "    Summary_results = pandas.DataFrame(index=Summary_index, columns=Summary_columns)\n",
    "\n",
    "    # performance of the global model\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[0]] = round(r2_score(ytrain_true, ytrain_pred), 4)\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[1]] = round(root_mean_squared_error(ytrain_true, ytrain_pred), 4)\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[2]] = round(mean_squared_error(ytrain_true, ytrain_pred), 4)\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[3]] = Gibbs_risk_set[0]\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[4]] = risk_bound_set[0]\n",
    "    \n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[0]] = round(r2_score(yval_true, yval_pred), 4)\n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[1]] = round(root_mean_squared_error(yval_true, yval_pred), 4)\n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[2]] = round(mean_squared_error(yval_true, yval_pred), 4)\n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[3]] = Gibbs_risk_set[1]\n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[4]] = risk_bound_set[1]\n",
    "    \n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[0]] = round(r2_score(ytest_true, ytest_pred), 4)\n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[1]] = round(root_mean_squared_error(ytest_true, ytest_pred), 4)\n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[2]] = round(mean_squared_error(ytest_true, ytest_pred), 4)\n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[3]] = Gibbs_risk_set[2]\n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[4]] = risk_bound_set[2]\n",
    "    \n",
    "    return Summary_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
